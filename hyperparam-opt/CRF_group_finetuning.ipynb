{"cells":[{"cell_type":"markdown","metadata":{"id":"OKtTUPrFaysN"},"source":["### Install requirements for smooth run"]},{"cell_type":"markdown","metadata":{"id":"32ueMluQa1qU"},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"89ZybXBAZEvS"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import spacy\n","import nltk\n","import csv\n","import seaborn as sns\n","\n","import sklearn_crfsuite\n","from sklearn_crfsuite import metrics\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from sklearn.metrics import multilabel_confusion_matrix\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import make_scorer\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","import scipy\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","# Ignore warnings in the code execution"]},{"cell_type":"markdown","metadata":{"id":"EW0RWxJWa3hV"},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rPaL3SRKZEvU"},"outputs":[],"source":["# function for creating the X and y for the CRF \n","def transform_to_crf(dataframe, strategy = 'LO', feature_group = [], base_group = []):\n","  \"\"\"dataframe = entire train or development set as pandas dataframe\n","  strategy = 'LO' if you want to leave a certain group out of the analysis\n","  strategy = 'ONLY' if you want to train solely on that list of features\n","  strategy = 'STACK' if you want to stack the basegroup to the list of features \n","  feature_group = the list of features on which the strategy is performed\n","  If you run transform_to_crf(dataframe) it will automatically perform on all features\"\"\"\n","  \n","\n","  column_values = dataframe[['annotator']].values.ravel()\n","  annotator_ids = pd.unique(column_values)\n","  total_cols = ['token_no_stop','lemma','pos','prev_lemma','next_lemma','prev_pos','next_pos','snowball_stemmer',\n","                'porter_stemmer','head','dependency','is_part_of_negation','has_prefix','has_postfix','has_infix',\n","                'base_in_dictionary','has_apostrophe']\n","\n","  # print(dataframe.columns)\n","  # print('total_cols', total_cols)\n","  if strategy == 'LO':\n","    cols = [x for x in total_cols if x not in feature_group]\n","  elif strategy == 'ONLY':\n","    cols = feature_group\n","    print(cols)\n","  elif strategy == 'STACK':\n","    cols_dupl = base_group + feature_group\n","    cols = list(set(cols_dupl))\n","\n","\n","  X_ready = []\n","  y_ready = []\n","\n","  for annotator in annotator_ids:  # for each annotator\n","      # get the data for the annotator\n","      annotator_data = dataframe[dataframe['annotator'] == annotator]\n","      # get the sentence ids\n","      column_values = annotator_data[['sentence_id']].values.ravel()\n","      sentence_ids = pd.unique(column_values)  # get the unique sentence ids\n","\n","      for sent_id in sentence_ids:\n","        new_sentence = []\n","        sentence = annotator_data.loc[annotator_data['sentence_id']\n","                                            == sent_id]        \n","        y = sentence['label'].values.ravel()\n","        # Make y element str\n","        y = [str(x) for x in y]\n","        for i, token_features in sentence.iterrows():\n","          new_sentence.append(token_features[cols].to_dict())\n","        X_ready.append(new_sentence)\n","        y_ready.append(list(y))\n","\n","\n","  return X_ready, y_ready, cols"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"AJ95VtoPZEvV"},"outputs":[],"source":["# function for creating the CRF classifier and predict the labels\n","def predictions_crf(X_train, y_train, X_test):\n","\n","    crf = sklearn_crfsuite.CRF(\n","    algorithm='lbfgs', \n","    c1=0.1, \n","    c2=0.1, \n","    max_iterations=100, \n","    all_possible_transitions=True\n","    )\n","\n","    try:\n","        crf.fit(X_train, y_train)\n","    except AttributeError:\n","        pass\n","\n","    predictions = crf.predict(X_test)\n","    \n","    return crf, predictions"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"K6Dtr3dsb-93"},"outputs":[],"source":["def hyperparameter_search_crf(X_train, y_train, X_test):\n","\n","    crf = sklearn_crfsuite.CRF(\n","    algorithm='lbfgs', \n","    c1=0.1, \n","    c2=0.1, \n","    max_iterations=200, \n","    all_possible_transitions=True\n","    )\n","\n","    params_space = { \n","    'c1': scipy.stats.expon(scale=0.5),\n","    'c2': scipy.stats.expon(scale=0.05),\n","    }\n","\n","    crf.fit(X_train, y_train)\n","\n","    from sklearn_crfsuite import metrics\n","    labels = list(crf.classes_)\n","\n","    # use the same metric for evaluation\n","    f1_scorer = make_scorer(metrics.flat_f1_score,\n","                        average='weighted', labels=labels)\n","\n","    # search\n","    rs = RandomizedSearchCV(crf, params_space,\n","                            cv=3,\n","                            verbose=1,\n","                            n_jobs=-1,\n","                            n_iter=20,\n","                            scoring=f1_scorer)\n","    rs.fit(X_train, y_train)\n","\n","    # try:\n","    #     crf.fit(X_train, y_train)\n","    # except AttributeError:\n","    #     pass\n","\n","    predictions = rs.predict(X_test)\n","    \n","    return rs, predictions"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lKwaDpFaZEvW"},"outputs":[],"source":["# function for extracting the f1 and confusion matrix from the predictions\n","def get_metrics(predictions, y_test, crf):\n","    from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n","    labels = list(crf.classes_)\n","    f1 = metrics.flat_f1_score(y_test, predictions,\n","                        average='macro', labels=labels)\n","    y_test_flatten = [y for x in y_test for y in x]\n","    predictions_flatten = [y for x in predictions for y in x]\n","    cm = confusion_matrix(y_test_flatten, predictions_flatten, labels)\n","    print('best params:', crf.best_params_)\n","    print('model size: {:0.2f}'.format(crf.best_estimator_.size_ / 1000000))\n","    return cm, f1\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"r9uLpURoZEvW"},"outputs":[],"source":["# function for extracting all confusion matrices, f1's and prediction lists for every group of features\n","def run_feature_analysis(strat = 'LO', base_group = []):\n","    \"\"\"strat = 'LO' if you want to leave a certain group out of the analysis\n","    strat = 'ONLY' if you want to train solely on that list of features \n","    strat = 'STACK' if you want to increase the base_group \n","    default is 'LO' \"\"\"\n","    \n","    print('strategy: ', strat)\n","    \n","    feature_groups = [['token_no_stop'],['lemma','pos','snowball_stemmer','porter_stemmer'],['next_lemma','next_pos'], \n","    ['prev_lemma', 'prev_pos'], ['head','dependency'], ['is_part_of_negation'], ['has_prefix','has_postfix','has_infix'],\n","    ['base_in_dictionary'],['has_apostrophe']]\n","\n","    cms = []\n","    f1s = []\n","    predictions_list = []\n","    cs  = []\n","\n","    t = train.copy()\n","    d = dev.copy()\n","\n","    for i, group in enumerate(feature_groups):\n","        X_train, y_train, cols = transform_to_crf(t,  strategy = strat, feature_group=group, base_group= base_group)\n","        X_test, y_test, cols = transform_to_crf(d, strategy = strat, feature_group=group, base_group = base_group)\n","        crf, predictions = predictions_crf(X_train, y_train, X_test)\n","        cm, f1 = get_metrics(predictions, y_test, crf)\n","        cms.append(cm)\n","        f1s.append(f1)\n","        predictions_list.append(predictions)\n","        cs.append(crf.best_params_)\n","        print(cols, ' has f1 score of ', f1)\n","        \n","    return cms, f1s, predictions_list, cs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def run_feature_analysis_with_hyperparameter_opt_extra(strat = 'LO', base_group = []):\n","    \"\"\"strat = 'LO' if you want to leave a certain group out of the analysis\n","    strat = 'ONLY' if you want to train solely on that list of features \n","    strat = 'STACK' if you want to increase the base_group \n","    default is 'LO' \"\"\"\n","    \n","    print('strategy: ', strat)\n","    \n","    feature_groups = [['lemma','pos','snowball_stemmer','porter_stemmer','is_part_of_negation', 'has_prefix','has_postfix','has_infix']]\n","\n","    cms = []\n","    f1s = []\n","    css = []\n","    predictions_list = []\n","\n","    for i, group in enumerate(feature_groups):\n","        X_train, y_train, cols = transform_to_crf(train,  strategy = strat, feature_group=group, base_group= base_group)\n","        X_test, y_test, cols = transform_to_crf(dev, strategy = strat, feature_group=group, base_group = base_group)\n","        crf, predictions = hyperparameter_search_crf(X_train, y_train, X_test)\n","        cm, f1 = get_metrics(predictions, y_test, crf)\n","        cms.append(cm)\n","        f1s.append(f1)\n","        css.append(crf.best_params_)\n","        predictions_list.append(predictions)\n","        if strat == \"STACK\": base_group = cols\n","        print(cols, ' has f1 score of ', f1)\n","        \n","    return cms, f1s, predictions_list, css, crf"]},{"cell_type":"markdown","metadata":{"id":"7Nwg6R8OMsz_"},"source":["## SEM 2012 Corpus"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":884,"status":"ok","timestamp":1643445893756,"user":{"displayName":"Julio Lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5Hu5YJ4IoO3zHcXwjWdiTMRYS7xnn6dc-s_XjoQ=s64","userId":"09330088662129574823"},"user_tz":-60},"id":"SZAYTfeRZXCF","outputId":"fb54d03e-5a06-4825-99e4-8ac3472ee598"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>annotator</th>\n","      <th>sentence_id</th>\n","      <th>token_id</th>\n","      <th>token</th>\n","      <th>label</th>\n","      <th>token_lower</th>\n","      <th>token_no_punct</th>\n","      <th>token_no_stop</th>\n","      <th>lemma</th>\n","      <th>pos</th>\n","      <th>...</th>\n","      <th>porter_stemmer</th>\n","      <th>head</th>\n","      <th>dependency</th>\n","      <th>is_part_of_negation</th>\n","      <th>has_prefix</th>\n","      <th>has_postfix</th>\n","      <th>has_infix</th>\n","      <th>base</th>\n","      <th>base_in_dictionary</th>\n","      <th>has_apostrophe</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>baskervilles01</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>chapter</td>\n","      <td>O</td>\n","      <td>chapter</td>\n","      <td>chapter</td>\n","      <td>chapter</td>\n","      <td>chapter</td>\n","      <td>NOUN</td>\n","      <td>...</td>\n","      <td>chapter</td>\n","      <td>5</td>\n","      <td>nmod</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>chapter</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>baskervilles01</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1.</td>\n","      <td>O</td>\n","      <td>1.</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>X</td>\n","      <td>...</td>\n","      <td>1.</td>\n","      <td>1</td>\n","      <td>nummod</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>1</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>baskervilles01</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>mr.</td>\n","      <td>O</td>\n","      <td>mr.</td>\n","      <td>mr</td>\n","      <td>mr</td>\n","      <td>mr</td>\n","      <td>PROPN</td>\n","      <td>...</td>\n","      <td>mr.</td>\n","      <td>5</td>\n","      <td>compound</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>mr</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>baskervilles01</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>sherlock</td>\n","      <td>O</td>\n","      <td>sherlock</td>\n","      <td>sherlock</td>\n","      <td>sherlock</td>\n","      <td>sherlock</td>\n","      <td>NOUN</td>\n","      <td>...</td>\n","      <td>sherlock</td>\n","      <td>5</td>\n","      <td>compound</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>sherlock</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>baskervilles01</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>holmes</td>\n","      <td>O</td>\n","      <td>holmes</td>\n","      <td>holmes</td>\n","      <td>holmes</td>\n","      <td>holmes</td>\n","      <td>PROPN</td>\n","      <td>...</td>\n","      <td>holm</td>\n","      <td>0</td>\n","      <td>ROOT</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>holmes</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 25 columns</p>\n","</div>"],"text/plain":["        annotator  sentence_id  token_id     token label token_lower  \\\n","0  baskervilles01            0         0   chapter     O     chapter   \n","1  baskervilles01            0         1        1.     O          1.   \n","2  baskervilles01            0         2       mr.     O         mr.   \n","3  baskervilles01            0         3  sherlock     O    sherlock   \n","4  baskervilles01            0         4    holmes     O      holmes   \n","\n","  token_no_punct token_no_stop     lemma    pos  ... porter_stemmer head  \\\n","0        chapter       chapter   chapter   NOUN  ...        chapter    5   \n","1              1             1         1      X  ...             1.    1   \n","2             mr            mr        mr  PROPN  ...            mr.    5   \n","3       sherlock      sherlock  sherlock   NOUN  ...       sherlock    5   \n","4         holmes        holmes    holmes  PROPN  ...           holm    0   \n","\n","  dependency is_part_of_negation has_prefix has_postfix  has_infix      base  \\\n","0       nmod                   0      False       False      False   chapter   \n","1     nummod                   0      False       False      False         1   \n","2   compound                   0      False       False      False        mr   \n","3   compound                   0      False       False      False  sherlock   \n","4       ROOT                   0      False       False      False    holmes   \n","\n","   base_in_dictionary  has_apostrophe  \n","0                True           False  \n","1               False           False  \n","2                True           False  \n","3                True           False  \n","4               False           False  \n","\n","[5 rows x 25 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Previous path: /content/drive/MyDrive/ATM /SEM2012_training_data_with_features.csv\n","# Change it to run it in a different drive\n","file_path =  'ATM/SEM2012_training_data_with_features.csv'\n","train = pd.read_csv(file_path, sep=\",\", header=0)\n","\n","\n","file_path_test =  'ATM/SEM2012_validation_data_with_features.csv'\n","dev = pd.read_csv(file_path_test, sep=\",\", header=0)\n","\n","train.head(5)"]},{"cell_type":"markdown","metadata":{"id":"eNa_In1ibdEe"},"source":["#### Run hyperparameter optimization with \"ONLY\" Strategy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1808916,"status":"ok","timestamp":1643417955178,"user":{"displayName":"Julio Lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5Hu5YJ4IoO3zHcXwjWdiTMRYS7xnn6dc-s_XjoQ=s64","userId":"09330088662129574823"},"user_tz":-60},"id":"zKxZvQB3q97R","outputId":"0fc929a5-8729-49ef-d83e-eb5a71b8a441"},"outputs":[],"source":["# perform feature ablagation study\n","cms_only1, f1s_only1, predictions_list_only1, cs_ONLY1, model = run_feature_analysis_with_hyperparameter_opt_extra(strat='ONLY')"]},{"cell_type":"markdown","metadata":{},"source":["best params: {'c1': 0.2160021395340066, 'c2': 0.0013076159888168548}\n","model size: 0.04\n","['lemma', 'pos', 'snowball_stemmer', 'porter_stemmer', 'is_part_of_negation', 'has_prefix', 'has_postfix', 'has_infix']  has f1 score of  0.8941584132126073"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["RandomizedSearchCV(cv=3,\n","                   estimator=CRF(algorithm='lbfgs',\n","                                 all_possible_transitions=True, c1=0.1, c2=0.1,\n","                                 keep_tempfiles=None, max_iterations=200),\n","                   n_iter=20, n_jobs=-1,\n","                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f1aeb525520>,\n","                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f1aea2af9a0>},\n","                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['O', 'B-NEG', 'I-NEG']),\n","                   verbose=1)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"Rf1LVAnQnPA8"},"source":["## Execution for BioCorpus"]},{"cell_type":"markdown","metadata":{"id":"8xvm7YJoHF3N"},"source":["Adapt the functions to the new Corpus as it does not have the head, dependency pair"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":270},"executionInfo":{"elapsed":1336,"status":"ok","timestamp":1643409451374,"user":{"displayName":"Julio Lopez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi5Hu5YJ4IoO3zHcXwjWdiTMRYS7xnn6dc-s_XjoQ=s64","userId":"09330088662129574823"},"user_tz":-60},"id":"Lf4JM8cq6G11","outputId":"492f26c2-4050-48a8-e695-70a79089a861"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>annotator</th>\n","      <th>sentence_id</th>\n","      <th>token_id</th>\n","      <th>token</th>\n","      <th>label</th>\n","      <th>token_lower</th>\n","      <th>token_no_punct</th>\n","      <th>token_no_stop</th>\n","      <th>lemma</th>\n","      <th>pos</th>\n","      <th>...</th>\n","      <th>porter_stemmer</th>\n","      <th>head</th>\n","      <th>dependency</th>\n","      <th>is_part_of_negation</th>\n","      <th>has_prefix</th>\n","      <th>has_postfix</th>\n","      <th>has_infix</th>\n","      <th>base</th>\n","      <th>base_in_dictionary</th>\n","      <th>has_apostrophe</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>10092801</td>\n","      <td>6</td>\n","      <td>10</td>\n","      <td>adenovirus</td>\n","      <td>O</td>\n","      <td>adenovirus</td>\n","      <td>adenovirus</td>\n","      <td>adenovirus</td>\n","      <td>adenovirus</td>\n","      <td>PROPN</td>\n","      <td>...</td>\n","      <td>adenoviru</td>\n","      <td>3</td>\n","      <td>attr</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>denovirus</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>91187152</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>environmental</td>\n","      <td>O</td>\n","      <td>environmental</td>\n","      <td>environmental</td>\n","      <td>environmental</td>\n","      <td>environmental</td>\n","      <td>ADJ</td>\n","      <td>...</td>\n","      <td>environment</td>\n","      <td>7</td>\n","      <td>amod</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>environmental</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>7522257</td>\n","      <td>10</td>\n","      <td>7</td>\n","      <td>cd14</td>\n","      <td>O</td>\n","      <td>cd14</td>\n","      <td>cd14</td>\n","      <td>cd14</td>\n","      <td>cd14</td>\n","      <td>PROPN</td>\n","      <td>...</td>\n","      <td>cd14</td>\n","      <td>8</td>\n","      <td>nmod</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>cd14</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>92179751</td>\n","      <td>3</td>\n","      <td>78</td>\n","      <td>incubation</td>\n","      <td>O</td>\n","      <td>incubation</td>\n","      <td>incubation</td>\n","      <td>incubation</td>\n","      <td>incubation</td>\n","      <td>NOUN</td>\n","      <td>...</td>\n","      <td>incub</td>\n","      <td>59</td>\n","      <td>dobj</td>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>cubation</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7541794</td>\n","      <td>5</td>\n","      <td>41</td>\n","      <td>for</td>\n","      <td>O</td>\n","      <td>for</td>\n","      <td>for</td>\n","      <td>for</td>\n","      <td>for</td>\n","      <td>ADP</td>\n","      <td>...</td>\n","      <td>for</td>\n","      <td>28</td>\n","      <td>prep</td>\n","      <td>0</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>False</td>\n","      <td>for</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 25 columns</p>\n","</div>"],"text/plain":["  annotator  sentence_id  token_id          token label    token_lower  \\\n","0  10092801            6        10     adenovirus     O     adenovirus   \n","1  91187152            3         7  environmental     O  environmental   \n","2   7522257           10         7           cd14     O           cd14   \n","3  92179751            3        78     incubation     O     incubation   \n","4   7541794            5        41            for     O            for   \n","\n","  token_no_punct  token_no_stop          lemma    pos  ... porter_stemmer  \\\n","0     adenovirus     adenovirus     adenovirus  PROPN  ...      adenoviru   \n","1  environmental  environmental  environmental    ADJ  ...    environment   \n","2           cd14           cd14           cd14  PROPN  ...           cd14   \n","3     incubation     incubation     incubation   NOUN  ...          incub   \n","4            for            for            for    ADP  ...            for   \n","\n","  head dependency is_part_of_negation has_prefix has_postfix  has_infix  \\\n","0    3       attr                   0       True       False      False   \n","1    7       amod                   0      False       False      False   \n","2    8       nmod                   0      False       False      False   \n","3   59       dobj                   0       True       False      False   \n","4   28       prep                   0      False       False      False   \n","\n","            base  base_in_dictionary  has_apostrophe  \n","0      denovirus               False           False  \n","1  environmental                True           False  \n","2           cd14               False           False  \n","3       cubation               False           False  \n","4            for                True           False  \n","\n","[5 rows x 25 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["file_path =  'biocorpus_training_data_with_features.csv'\n","train = pd.read_csv(file_path, sep=\",\", header=0, dtype={\"annotator\": \"string\", \"sentence_id\": int, \"token_id\":int})\n","train\n","\n","\n","file_path_test =  'biocorpus_validation_data_with_features.csv'\n","dev = pd.read_csv(file_path_test, sep=\",\", header=0)\n","\n","train.head(5)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# function for creating the CRF classifier and predict the labels\n","def get_predictions(X_train, y_train, X_test, model):\n","\n","    predictions = model.predict(X_test)\n","    \n","    return predictions"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['lemma', 'pos', 'snowball_stemmer', 'porter_stemmer', 'is_part_of_negation', 'has_prefix', 'has_postfix', 'has_infix']\n","['lemma', 'pos', 'snowball_stemmer', 'porter_stemmer', 'is_part_of_negation', 'has_prefix', 'has_postfix', 'has_infix']\n"]}],"source":["group = ['lemma','pos','snowball_stemmer','porter_stemmer','is_part_of_negation', 'has_prefix','has_postfix','has_infix']\n","X_train, y_train, cols = transform_to_crf(train,  strategy = \"ONLY\", feature_group=group, base_group= [])\n","X_test, y_test, cols = transform_to_crf(dev, strategy = \"ONLY\", feature_group=group, base_group = [])\n","preds = get_predictions(X_train, y_train, X_test, model)\n","get_metrics(preds, y_test, model)"]},{"cell_type":"markdown","metadata":{},"source":["best params: {'c1': 0.2160021395340066, 'c2': 0.0013076159888168548}\n","model size: 0.04\n","0.33291237949333724"]}],"metadata":{"accelerator":"TPU","colab":{"collapsed_sections":["lV-ZFkRGbORS"],"name":"CRF_group_finetuning.ipynb","provenance":[]},"interpreter":{"hash":"b0658e4561313c79abdb6145ee817af4de56f1cefe416636511aae6283741994"},"kernelspec":{"display_name":"Python 3.8.11 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
